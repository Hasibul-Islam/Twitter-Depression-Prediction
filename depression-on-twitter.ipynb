{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "We use the [sentiment140 dataset](https://www.kaggle.com/kazanova/sentiment140). It contains 1,600,000 tweets extracted using the twitter api. The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment.\n",
    "\n",
    "It contains the following 6 fields:\n",
    "\n",
    "1. target: the polarity of the tweet (0 = negative, 1 = positive)\n",
    "2. ids: The id of the tweet ( 2087)\n",
    "3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "5. user: the user that tweeted (robotickilldozr)\n",
    "6. text: the text of the tweet (Lyx is cool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Specially for kaggle notebook\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('.../'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../\",encoding='latin-1') #give your dataset directory here\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n",
    "data.columns = DATASET_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a subset of data to speed it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "data_ = copy.deepcopy(data)\n",
    "\n",
    "positif_data = data_[data_.target==4].iloc[:80000,:]\n",
    "negative_data = data_[data_.target==0].iloc[:80000,:]\n",
    "\n",
    "sub_data = pd.concat([positif_data,negative_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot(column='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_target=data.groupby('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date\n",
    "\n",
    "At what time do people like to tweet? Is there a clear link between the time of tweeting and the emotion of the content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = {'target': data['target'], 'date': data['date']}\n",
    "df = pd.DataFrame(data_)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets ensure the 'date' column is in date format\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = [ df['date'][i].hour for i in range(len(df['date'])) ]\n",
    "df['hour'] = hour\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_data = {'0': [0]*24, '2': [0]*24, '4': [0]*24}\n",
    "for i in range(len(df['hour'])):\n",
    "    target = str(df['target'][i])\n",
    "    hour = int(df['hour'][i])\n",
    "    hour_data[target][hour] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_data = [hour_data['0'], hour_data['2'], hour_data['4']]\n",
    "# Transpose\n",
    "hour_data = list(map(list,zip(*hour_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(hour_data,index = [i for i in range(24)],columns=['negative', 'neutral', 'positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_at_count = 0\n",
    "negative_at_count = 0\n",
    "TweetTextList = list(sub_data['TweetText'])\n",
    "targetList = list(sub_data['target'])\n",
    "for i in range(len(sub_data['TweetText'])):\n",
    "    if TweetTextList[i].find('@') != -1:\n",
    "        if targetList[i] == 4:\n",
    "            positive_at_count += 1\n",
    "        else:\n",
    "            negative_at_count += 1\n",
    "at_counts = [positive_at_count, negative_at_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "names = ['positive', 'negative']\n",
    "values = [positive_at_count, negative_at_count]\n",
    "plt.bar(names, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words\n",
    "\n",
    "Words distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "newdata = copy.deepcopy(sub_data)\n",
    "newdata.drop(['ids','date','flag','user'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common words in dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in newdata['TweetText']])\n",
    "wordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(all_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common positive words in dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = ' '.join([text for text in data['TweetText'][data['target']==4]])\n",
    "wordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(positive_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the most common negative words in dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_words = ' '.join([text for text in data['TweetText'][data['target']==0]])\n",
    "wordcloud = WordCloud(width=800,height=500,random_state=21,max_font_size=110).generate(negative_words)\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.imshow(wordcloud,interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positif_data = data[data.target==4].iloc[:10000,:]\n",
    "print(positif_data.shape)\n",
    "negative_data = data[data.target==0].iloc[:10000,:]\n",
    "print(negative_data.shape)\n",
    "data = pd.concat([positif_data,negative_data],axis = 0)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Twitter Handles (@user)\n",
    "data['Clean_TweetText'] = data['TweetText'].str.replace(\"@\", \"\") \n",
    "# Removing links\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(r\"http\\S+\", \"\") \n",
    "# Removing Punctuations, Numbers, and Special Characters\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "# Remove stop words\n",
    "import nltk\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return clean_text\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda text : remove_stopwords(text.lower()))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Tokenization and Normalization\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: nltk.word_tokenize(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now letâ€™s stitch these tokens back together\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x]))\n",
    "# Removing small words\n",
    "data['Clean_TweetText'] = data['Clean_TweetText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english') \n",
    "cv = count_vectorizer.fit_transform(data['Clean_TweetText'])\n",
    "cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(cv,data['target'] , test_size=.2,stratify=data['target'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBC\n",
    "xgbc = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3)\n",
    "xgbc.fit(X_train,y_train)\n",
    "prediction_xgb = xgbc.predict(X_test)\n",
    "print(accuracy_score(prediction_xgb,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest\n",
    "rf = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rf.fit(X_train,y_train)\n",
    "prediction_rf = rf.predict(X_test)\n",
    "print(accuracy_score(prediction_rf,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
